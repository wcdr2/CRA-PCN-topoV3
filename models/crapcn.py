import os
import torch
import copy
import math
import numpy as np
from torch import nn, einsum
from pointnet2_ops.pointnet2_utils import furthest_point_sample, gather_operation, ball_query, three_nn, three_interpolate, grouping_operation
import torch.nn.functional as F
from torch import einsum
from models.utils import PointNet_SA_Module_KNN, MLP_Res, MLP_CONV, fps_subsample, query_knn, grouping_operation, get_nearest_index, indexing_neighbor

from models.topo_module_v2 import TopoReasoningBlockV2

from esp import EspAttention



# VA from Point Transformer
class VectorAttention(nn.Module):
    def __init__(self, in_channel = 128, dim = 64, n_knn = 16, attn_hidden_multiplier = 4):
        super().__init__()
        self.n_knn = n_knn
        self.conv_key = nn.Conv1d(in_channel, dim, 1)
        self.conv_query = nn.Conv1d(in_channel, dim, 1)
        self.conv_value = nn.Conv1d(in_channel, dim, 1)
        self.pos_mlp = nn.Sequential(
            nn.Conv2d(3, dim, 1),
            nn.BatchNorm2d(dim),
            nn.ReLU(),
            nn.Conv2d(dim, dim, 1)
        )
        self.attn_mlp = nn.Sequential(
            nn.Conv2d(dim, dim * attn_hidden_multiplier, 1),
            nn.BatchNorm2d(dim * attn_hidden_multiplier),
            nn.ReLU(),
            nn.Conv2d(dim * attn_hidden_multiplier, dim, 1)
        )
        self.conv_end = nn.Conv1d(dim, in_channel, 1)

    def forward(self, query, support):
        pq, fq = query
        ps, fs = support

        identity = fq 
        query, key, value = self.conv_query(fq), self.conv_key(fs), self.conv_value(fs) 
        
        B, D, N = query.shape

        pos_flipped_1 = ps.permute(0, 2, 1).contiguous() 
        pos_flipped_2 = pq.permute(0, 2, 1).contiguous() 
        idx_knn = query_knn(self.n_knn, pos_flipped_1, pos_flipped_2)

        key = grouping_operation(key, idx_knn) 
        qk_rel = query.reshape((B, -1, N, 1)) - key  

        pos_rel = pq.reshape((B, -1, N, 1)) - grouping_operation(ps, idx_knn)  
        pos_embedding = self.pos_mlp(pos_rel) 

        attention = self.attn_mlp(qk_rel + pos_embedding) 
        attention = torch.softmax(attention, -1)

        value = grouping_operation(value, idx_knn) + pos_embedding  
        agg = einsum('b c i j, b c i j -> b c i', attention, value)  
        output = self.conv_end(agg) + identity
        
        return output

def hierarchical_fps(pts, rates):
    pts_flipped = pts.permute(0, 2, 1).contiguous()
    B, _, N = pts.shape
    now = N
    fps_idxs = []
    for i in range(len(rates)):
        now = now // rates[i]
        if now == N:
            fps_idxs.append(None)
        else:
            fps_idxs.append(furthest_point_sample(pts_flipped, now))
    return fps_idxs

# project f from p1 onto p2
def three_inter(f, p1, p2):
    # print(f.shape, p1.shape, p2.shape)
    # p1_flipped = p1.permute(0, 2, 1).contiguous()
    # p2_flipped = p2.permute(0, 2, 1).contiguous()
    idx, dis = get_nearest_index(p2, p1, k=3, return_dis=True) 
    dist_recip = 1.0 / (dis + 1e-8)
    norm = torch.sum(dist_recip, dim = 2, keepdim = True) 
    weight = dist_recip / norm
    proj_f = torch.sum(indexing_neighbor(f, idx) * weight.unsqueeze(1), dim=-1)
    return proj_f

# Cross-Resolution Transformer
class CRT(nn.Module):
    def __init__(self, dim_in = 128, is_inter = True, down_rates = [1, 4, 2], knns = [16, 12, 8]):
        super().__init__()
        self.down_rates = down_rates
        self.is_inter = is_inter
        self.num_scale = len(down_rates)

        self.attn_lists = nn.ModuleList()
        self.q_mlp_lists = nn.ModuleList()
        self.s_mlp_lists = nn.ModuleList()
        for i in range(self.num_scale):
            self.attn_lists.append(VectorAttention(in_channel = dim_in, dim = 64, n_knn = knns[i]))

        for i in range(self.num_scale - 1):
            self.q_mlp_lists.append(MLP_Res(in_dim = 128*2, hidden_dim = 128, out_dim = 128))
            self.s_mlp_lists.append(MLP_Res(in_dim = 128*2, hidden_dim = 128, out_dim = 128))

    def forward(self, query, support, fps_idxs_q = None, fps_idxs_s = None):
        pq, fq = query
        ps, fs = support
        # prepare fps_idxs_q and fps_idxs_s
        if fps_idxs_q == None:
            fps_idxs_q = hierarchical_fps(pq, self.down_rates)
        
        if fps_idxs_s == None:
            if self.is_inter:
                fps_idxs_s = hierarchical_fps(ps, self.down_rates) # inter-level
            else:
                fps_idxs_s = fps_idxs_q # intra-level
        
        # top-down aggregation
        pre_f = None
        pre_pos = None
        
        for i in range(self.num_scale - 1, -1, -1):
            if fps_idxs_q[i] == None:
                _pos1 = pq
            else:
                _pos1 = gather_operation(pq, fps_idxs_q[i])
            
            if fps_idxs_s[i] == None:
                _pos2 = ps
            else:
                _pos2 = gather_operation(ps, fps_idxs_s[i])

            if i == self.num_scale - 1:
                if fps_idxs_q[i] == None:
                    _f1 = fq
                else:
                    _f1 = gather_operation(fq, fps_idxs_q[i])
                if fps_idxs_s[i] == None:
                    _f2 = fs
                else:
                    _f2 = gather_operation(fs, fps_idxs_s[i])   
                
            else: 
                proj_f1 = three_inter(pre_f, pre_pos, _pos1)
                proj_f2 = three_inter(pre_f, pre_pos, _pos2)
                if fps_idxs_q[i] == None:
                    _f1 = fq
                else:
                    _f1 = gather_operation(fq, fps_idxs_q[i])
                if fps_idxs_s[i] == None:
                    _f2 = fs
                else:
                    _f2 = gather_operation(fs, fps_idxs_s[i]) 
                
                _f1 = self.q_mlp_lists[i](torch.cat([_f1, proj_f1], dim = 1))
                _f2 = self.s_mlp_lists[i](torch.cat([_f2, proj_f2], dim = 1))

            f = self.attn_lists[i]([_pos1, _f1], [_pos2, _f2])

            pre_f = f
            pre_pos = _pos1
        
        agg_f = pre_f
        return agg_f, fps_idxs_q, fps_idxs_s

# encoder
class Encoder(nn.Module):
    def __init__(self, out_dim = 512, n_knn = 16):
        super().__init__()
        self.sa_module_1 = PointNet_SA_Module_KNN(512, 16, 3, [64, 128], group_all = False, if_bn = False, if_idx = True)
        self.crt_1 = CRT(dim_in = 128, is_inter = False, down_rates = [1, 2, 2], knns = [16, 12, 8])
        
        self.sa_module_2 = PointNet_SA_Module_KNN(128, 16, 128, [128, 256], group_all = False, if_bn = False, if_idx = True)
        self.conv_21 = nn.Conv1d(256, 128, 1)
        self.crt_2 = CRT(dim_in = 128, is_inter = False, down_rates = [1, 2, 2], knns = [16, 12, 8])
        self.conv_22 = nn.Conv1d(128, 256, 1)

        self.sa_module_3 = PointNet_SA_Module_KNN(None, None, 256, [512, out_dim], group_all = True, if_bn = False)

    def forward(self, partial_cloud):
        l0_xyz = partial_cloud
        l0_points = partial_cloud

        l1_xyz, l1_points, _ = self.sa_module_1(l0_xyz, l0_points)  
        l1_points, _, _ = self.crt_1([l1_xyz, l1_points], [l1_xyz, l1_points], None, None)

        l2_xyz, l2_points, _ = self.sa_module_2(l1_xyz, l1_points)
        l2_points_dim128 = self.conv_21(l2_points)
        l2_points_dim128, _, _ = self.crt_2([l2_xyz, l2_points_dim128], [l2_xyz, l2_points_dim128], None, None)
        l2_points = self.conv_22(l2_points_dim128) + l2_points

        l3_xyz, l3_points = self.sa_module_3(l2_xyz, l2_points)  

        return l2_xyz, l2_points, l3_points

class UpTransformer(nn.Module):
    def __init__(self, in_channel=128, out_channel=128, dim=64, n_knn=20, up_factor=2,
                 pos_hidden_dim=64, attn_hidden_multiplier=4, scale_layer=nn.Softmax, attn_channel=True):
        super(UpTransformer, self).__init__()
        self.n_knn = n_knn
        self.up_factor = up_factor
        attn_out_channel = dim if attn_channel else 1

        self.conv_key = nn.Conv1d(in_channel, dim, 1)
        self.conv_query = nn.Conv1d(in_channel, dim, 1)
        self.conv_value = nn.Conv1d(in_channel, dim, 1)



        self.scale = scale_layer(dim=-1) if scale_layer is not None else nn.Identity()

        self.pos_mlp = nn.Sequential(
            nn.Conv2d(3, pos_hidden_dim, 1),
            nn.BatchNorm2d(pos_hidden_dim),
            nn.ReLU(),
            nn.Conv2d(pos_hidden_dim, dim, 1)
        )

        # attention layers
        self.attn_mlp = [nn.Conv2d(dim, dim * attn_hidden_multiplier, 1),
                         nn.BatchNorm2d(dim * attn_hidden_multiplier),
                         nn.ReLU()]
        if up_factor:
            self.attn_mlp.append(nn.ConvTranspose2d(dim * attn_hidden_multiplier, attn_out_channel, (up_factor,1), (up_factor,1)))
        else:
            self.attn_mlp.append(nn.Conv2d(dim * attn_hidden_multiplier, attn_out_channel, 1))
        self.attn_mlp = nn.Sequential(*self.attn_mlp)

        # upsample previous feature
        self.upsample1 = nn.Upsample(scale_factor=(up_factor,1)) if up_factor else nn.Identity()
        self.upsample2 = nn.Upsample(scale_factor=up_factor) if up_factor else nn.Identity()

        # residual connection
        self.conv_end = nn.Conv1d(dim, out_channel, 1)
        if in_channel != out_channel:
            self.residual_layer = nn.Conv1d(in_channel, out_channel, 1)
        else:
            self.residual_layer = nn.Identity()

    def forward(self, pos1, query, pos2, key):
        """
        Inputs:
            pos: (B, 3, N)
            key: (B, in_channel, N)
            query: (B, in_channel, N)
        """

        value = key # (B, dim, N)
        identity = query
        key = self.conv_key(key) # (B, dim, N)
        query = self.conv_query(query)
        value = self.conv_value(value)
        b, dim, n = query.shape

        pos1_flipped = pos1.permute(0, 2, 1).contiguous()
        pos2_flipped = pos2.permute(0, 2, 1).contiguous()
        idx_knn = query_knn(self.n_knn, pos2_flipped, pos1_flipped) # b, N1, k

        key = grouping_operation(key, idx_knn)  # (B, dim, N1, k)
        qk_rel = query.reshape((b, -1, n, 1)) - key

        pos_rel = pos1.reshape((b, -1, n, 1)) - grouping_operation(pos2, idx_knn)  # (B, 3, N, k)
        pos_embedding = self.pos_mlp(pos_rel)  # (B, dim, N, k)

        # attention
        attention = self.attn_mlp(qk_rel + pos_embedding) # (B, dim, N*up_factor, k)

        # softmax function
        attention = self.scale(attention)

        # knn value is correct
        value = grouping_operation(value, idx_knn) + pos_embedding # (B, dim, N, k)
        value = self.upsample1(value) # (B, dim, N*up_factor, k)

        agg = einsum('b c i j, b c i j -> b c i', attention, value)  # (B, dim, N*up_factor)
        y = self.conv_end(agg) # (B, out_dim, N*up_factor)

        # shortcut
        identity = self.residual_layer(identity) # (B, out_dim, N)
        identity = self.upsample2(identity) # (B, out_dim, N*up_factor)

        return y+identity

# seed generator using upsample transformer
class SeedGenerator(nn.Module):
    def __init__(self, feat_dim = 512, seed_dim = 128, n_knn = 16, factor = 2, attn_channel = True):
        super().__init__()
        self.uptrans = UpTransformer(in_channel = 256, out_channel = 128, dim = 64, n_knn = n_knn, attn_channel = attn_channel, up_factor = factor, scale_layer = None)
        self.mlp_1 = MLP_Res(in_dim = feat_dim + 128, hidden_dim = 128, out_dim = 128)
        self.mlp_2 = MLP_Res(in_dim = 128, hidden_dim = 64, out_dim = 128)
        self.mlp_3 = MLP_Res(in_dim = feat_dim + 128, hidden_dim = 128, out_dim = seed_dim)
        self.mlp_4 = nn.Sequential(
            nn.Conv1d(seed_dim, 64, 1),
            nn.ReLU(),
            nn.Conv1d(64, 3, 1)
        )

    def forward(self, feat, patch_xyz, patch_feat, partial):
        x1 = self.uptrans(patch_xyz, patch_feat, patch_xyz, patch_feat)  # (B, 128, 256)
        x1 = self.mlp_1(torch.cat([x1, feat.repeat((1, 1, x1.size(2)))], 1))
        x2 = self.mlp_2(x1)
        x3 = self.mlp_3(torch.cat([x2, feat.repeat((1, 1, x2.size(2)))], 1))  # (B, 128, 256)
        seed = self.mlp_4(x3)  # (B, 3, 256)
        x = fps_subsample(torch.cat([seed.permute(0, 2, 1).contiguous(), partial], dim=1), 512).permute(0, 2, 1).contiguous() # b, 3, 512
        return seed, x3, x

# seed generator using deconvolution
class SeedGenerator_Deconv(nn.Module):
    def __init__(self, feat_dim = 512, seed_dim = 128, n_knn = 16, factor = 2, attn_channel = True):
        super().__init__()
        num_pc = 256
        self.ps = nn.ConvTranspose1d(feat_dim, 128, num_pc, bias=True)
        self.mlp_1 = MLP_Res(in_dim = feat_dim + 128, hidden_dim = 128, out_dim = 128)
        self.mlp_2 = MLP_Res(in_dim = 128, hidden_dim = 64, out_dim = 128)
        self.mlp_3 = MLP_Res(in_dim = feat_dim + 128, hidden_dim = 128, out_dim = seed_dim)
        self.mlp_4 = nn.Sequential(
            nn.Conv1d(seed_dim, 64, 1),
            nn.ReLU(),
            nn.Conv1d(64, 3, 1)
        )

    def forward(self, feat, patch_xyz, patch_feat, partial):
        x1 = self.ps(feat)  # (B, 128, 256)
        x1 = self.mlp_1(torch.cat([x1, feat.repeat((1, 1, x1.size(2)))], 1))
        x2 = self.mlp_2(x1)
        x3 = self.mlp_3(torch.cat([x2, feat.repeat((1, 1, x2.size(2)))], 1))  # (B, 128, 256)
        seed = self.mlp_4(x3)  # (B, 3, 256)
        x = fps_subsample(torch.cat([seed.permute(0, 2, 1).contiguous(), partial], dim=1), 512).permute(0, 2, 1).contiguous() # b, 3, 512
        return seed, x3, x

# mini-pointnet
class PN(nn.Module):
    def __init__(self, feat_dim = 512):
        super().__init__()
        self.mlp_1 = MLP_CONV(in_channel = 3, layer_dims=[64, 128])
        self.mlp_2 = MLP_CONV(in_channel = 128 * 2 + feat_dim , layer_dims=[512, 256, 128])
        
    def forward(self, xyz, global_feat):
        b, _, n = xyz.shape
        feat = self.mlp_1(xyz)
        feat4cat = [feat, torch.max(feat, 2, keepdim=True)[0].repeat(1, 1, n), global_feat.repeat(1, 1, n)]
        point_feat = self.mlp_2(torch.cat(feat4cat, dim=1))
        return point_feat

class DeConv(nn.Module):
    def __init__(self, up_factor = 4):
        super().__init__()
        self.decrease_dim = MLP_CONV(in_channel = 128, layer_dims = [64, 32], bn = True)
        self.ps = nn.ConvTranspose1d(32, 128, up_factor, up_factor, bias = False)  
        self.mlp_res = MLP_Res(in_dim = 128 * 2, hidden_dim = 128, out_dim = 128)
        self.upper = nn.Upsample(scale_factor = up_factor)
        self.xyz_mlp = MLP_CONV(in_channel = 128, layer_dims = [64, 3])
    def forward(self, xyz, feat):
        feat_child = self.ps(self.decrease_dim(feat))
        feat_child = self.mlp_res(torch.cat([feat_child, self.upper(feat)], dim=1)) # b, 128, n*r
        delta = self.xyz_mlp(torch.relu(feat_child)) 
        new_xyz = self.upper(xyz) + torch.tanh(delta)
        return new_xyz


# upsampling block 
class UpBlock(nn.Module):
    def __init__(self, feat_dim = 512, down_rates = [1, 4, 2], knns = [16, 12, 8], up_factor = 4):
        super().__init__()
        self.pn = PN()
        self.inter_crt = CRT(dim_in = 128, is_inter = True, down_rates = down_rates, knns = knns)
        self.intra_crt = CRT(dim_in = 128, is_inter = False, down_rates = down_rates, knns = knns)
        self.deconv = DeConv(up_factor = up_factor)

    def forward(self, p, gf, pre, fps_idxs_1, fps_idxs_2):
        h = self.pn(p, gf)
        g, fps_idxs_q1, fps_idxs_s1 = self.inter_crt([p, h], pre, None, fps_idxs_1)
        f, _, _ = self.intra_crt([p, g], [p, g], fps_idxs_q1, fps_idxs_q1)
        new_xyz = self.deconv(p, f)
        return new_xyz, f, fps_idxs_q1, fps_idxs_s1

# decoder

class Decoder(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.ub0 = UpBlock(feat_dim = 512, down_rates = [1, 2, 2], knns = [16, 12, 8], up_factor = 1)
        self.ub1 = UpBlock(feat_dim = 512, down_rates = [1, 2, 2], knns = [16, 12, 8], up_factor = 4)
        self.ub2 = UpBlock(feat_dim = 512, down_rates = [1, 4, 2], knns = [16, 12, 8], up_factor = 8)

    def forward(self, global_f, p0, p_sd, f_sd):
        p1, f0, p0_fps_idxs_122, _ = self.ub0(p0, global_f, [p_sd, f_sd], None, None)
        p2, f1, p1_fps_idxs_122, _ = self.ub1(p1, global_f, [p0, f0], None, p0_fps_idxs_122)
        p3, _ , _______________, _ = self.ub2(p2, global_f, [p1, f1], None, None)
        
        all_pc = [p_sd.permute(0, 2, 1).contiguous(), p1.permute(0, 2, 1).contiguous(), \
            p2.permute(0, 2, 1).contiguous(), p3.permute(0, 2, 1).contiguous()]
        return all_pc

class Decoder_sn55(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.ub0 = UpBlock(feat_dim = 512, down_rates = [1, 2, 2], knns = [16, 12, 8], up_factor = 1)
        self.ub1 = UpBlock(feat_dim = 512, down_rates = [1, 2, 2], knns = [16, 12, 8], up_factor = 4)
        self.ub2 = UpBlock(feat_dim = 512, down_rates = [1, 4, 2], knns = [16, 12, 8], up_factor = 4)

    def forward(self, global_f, p0, p_sd, f_sd):
        p1, f0, p0_fps_idxs_122, _ = self.ub0(p0, global_f, [p_sd, f_sd], None, None)
        p2, f1, p1_fps_idxs_122, _ = self.ub1(p1, global_f, [p0, f0], None, p0_fps_idxs_122)
        p3, _ , _______________, _ = self.ub2(p2, global_f, [p1, f1], None, None)
        
        all_pc = [p_sd.permute(0, 2, 1).contiguous(), p1.permute(0, 2, 1).contiguous(), \
            p2.permute(0, 2, 1).contiguous(), p3.permute(0, 2, 1).contiguous()]
        return all_pc

class Decoder_mvp(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.ub0 = UpBlock(feat_dim = 512, down_rates = [1, 2, 2], knns = [16, 12, 8], up_factor = 1)
        self.ub1 = UpBlock(feat_dim = 512, down_rates = [1, 2, 2], knns = [16, 12, 8], up_factor = 4)

    def forward(self, global_f, p0, p_sd, f_sd):
        p1, f0, p0_fps_idxs_122, _ = self.ub0(p0, global_f, [p_sd, f_sd], None, None)
        p2, f1, _, __ = self.ub1(p1, global_f, [p0, f0], None, p0_fps_idxs_122)
        
        all_pc = [p_sd.permute(0, 2, 1).contiguous(), p1.permute(0, 2, 1).contiguous(), \
            p2.permute(0, 2, 1).contiguous()]
        return all_pc


# CRA-PCN 
class CRAPCN(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.encoder = Encoder()
        self.seed_generator = SeedGenerator()
        self.decoder = Decoder()

    def forward(self, xyz):
        pp, fp, global_f = self.encoder(xyz.permute(0, 2, 1).contiguous())
        p_sd, f_sd, p0 = self.seed_generator(global_f, pp, fp, xyz)
        all_pc = self.decoder(global_f, p0, p_sd, f_sd)
        return all_pc

class CRAPCN_sn55(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.encoder = Encoder()
        self.seed_generator = SeedGenerator()
        self.decoder = Decoder_sn55()

    def forward(self, xyz):
        pp, fp, global_f = self.encoder(xyz.permute(0, 2, 1).contiguous())
        p_sd, f_sd, p0 = self.seed_generator(global_f, pp, fp, xyz)
        all_pc = self.decoder(global_f, p0, p_sd, f_sd)
        return all_pc

class CRAPCN_mvp(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.encoder = Encoder()
        self.seed_generator = SeedGenerator()
        self.decoder = Decoder_mvp()

    def forward(self, xyz):
        pp, fp, global_f = self.encoder(xyz.permute(0, 2, 1).contiguous())
        p_sd, f_sd, p0 = self.seed_generator(global_f, pp, fp, xyz)
        all_pc = self.decoder(global_f, p0, p_sd, f_sd)
        return all_pc

# CRA-PCN 
class CRAPCN_d(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.encoder = Encoder()
        self.seed_generator = SeedGenerator_Deconv()
        self.decoder = Decoder()

    def forward(self, xyz):
        pp, fp, global_f = self.encoder(xyz.permute(0, 2, 1).contiguous())
        p_sd, f_sd, p0 = self.seed_generator(global_f, pp, fp, xyz)
        all_pc = self.decoder(global_f, p0, p_sd, f_sd)
        return all_pc

class CRAPCN_sn55_d(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.encoder = Encoder()
        self.seed_generator = SeedGenerator_Deconv()
        self.decoder = Decoder_sn55()

    def forward(self, xyz):
        pp, fp, global_f = self.encoder(xyz.permute(0, 2, 1).contiguous())
        p_sd, f_sd, p0 = self.seed_generator(global_f, pp, fp, xyz)
        all_pc = self.decoder(global_f, p0, p_sd, f_sd)
        return all_pc

class CRAPCN_mvp_d(nn.Module):
    def __init__(self, ):
        super().__init__()
        self.encoder = Encoder()
        self.seed_generator = SeedGenerator_Deconv()
        self.decoder = Decoder_mvp()

    def forward(self, xyz):
        pp, fp, global_f = self.encoder(xyz.permute(0, 2, 1).contiguous())
        p_sd, f_sd, p0 = self.seed_generator(global_f, pp, fp, xyz)
        all_pc = self.decoder(global_f, p0, p_sd, f_sd)
        return all_pc

class GraphTopoLayer(nn.Module):
    #è¿™ä¸ªç±»åŒ…å«ï¼šå›¾æ„å»º + å•å±‚æ¶ˆæ¯ä¼ é€’(å›¾æ³¨æ„åŠ›å—)
    
    """
    å›¾æ‹“æ‰‘æ¶ˆæ¯ä¼ é€’å±‚ï¼ˆGraphTopoLayerï¼‰

    åŠŸèƒ½ï¼š
      - åœ¨å½“å‰ç‚¹åæ ‡ xyz ä¸Šæ„å»º k è¿‘é‚»å›¾ï¼ˆkNN å›¾ï¼‰
      - ä½¿ç”¨èŠ‚ç‚¹ç‰¹å¾ h åœ¨å›¾ä¸Šåšä¸€æ¬¡â€œä¿¡æ¯ä¼ é€’â€ï¼ˆmessage passingï¼‰
      - é€šè¿‡æ®‹å·®è¿æ¥æ›´æ–°ç‰¹å¾ï¼šh_out = h + msg

    è¾“å…¥:
      xyz: (B, N, 3)   æ¯ä¸ªç‚¹çš„ä¸‰ç»´åæ ‡
      h:   (B, N, C)   æ¯ä¸ªç‚¹çš„éšè—ç‰¹å¾ï¼ˆhidden featureï¼‰

    è¾“å‡º:
      h_out: (B, N, C) æ›´æ–°åçš„éšè—ç‰¹å¾
    """
    def __init__(self, hidden_dim=128, k=16):
        super().__init__()
        self.k = k
        self.hidden_dim = hidden_dim

        # è¾¹ç‰¹å¾ç»´åº¦ = h_i (C) + (h_j - h_i)(C) + (x_j - x_i)(3) = 2C + 3
        edge_dim = hidden_dim * 2 + 3

        # ç”¨äºè®¡ç®—é‚»å±…â€œé‡è¦æ€§â€çš„ MLPï¼Œå¯¹åº”æ³¨æ„åŠ›åˆ†æ•° score_ij
        self.edge_mlp_attn = nn.Sequential(
            nn.Linear(edge_dim, edge_dim),
            nn.ReLU(inplace=True),
            nn.Linear(edge_dim, 1)        # è¾“å‡ºä¸€ä¸ªæ ‡é‡å¾—åˆ†
        )

        # ç”¨äºç”Ÿæˆé‚»å±…â€œå»ºè®®æ¶ˆæ¯â€çš„ MLPï¼Œå¯¹åº” m_ij
        self.edge_mlp_msg = nn.Sequential(
            nn.Linear(edge_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, xyz, h):
        """
        xyz: (B, N, 3)
        h:   (B, N, C)
        """
        from models.utils import query_knn, grouping_operation

        B, N, C = h.shape

        # 1) åœ¨ xyz ä¸Šæ„å»º kNN å›¾
        #    æ³¨æ„ï¼šquery_knn æœŸæœ›è¾“å…¥åæ ‡ä¸º (B,N,3)ï¼Œæ‰€ä»¥è¿™é‡Œä¸ç”¨è½¬ç½®
        idx_knn = query_knn(self.k, xyz, xyz)          # (B,N,k)

        # 2) å–é‚»å±…ç‰¹å¾
        #    h_t: (B,C,N)ï¼Œgrouping_operation è¾“å‡º (B,C,N,k)
        h_t = h.permute(0, 2, 1).contiguous()          # (B,C,N)
        neigh_h = grouping_operation(h_t, idx_knn)     # (B,C,N,k)
        neigh_h = neigh_h.permute(0, 2, 3, 1)          # (B,N,k,C)

        # ä¸­å¿ƒç‚¹ç‰¹å¾: (B,N,1,C) -> (B,N,k,C)
        center_h = h.unsqueeze(2).expand(-1, -1, self.k, -1)

        # 3) å–é‚»å±…åæ ‡
        #    xyz_t: (B,3,N)ï¼Œgrouping_operation è¾“å‡º (B,3,N,k)
        xyz_t = xyz.permute(0, 2, 1).contiguous()      # (B,3,N)
        neigh_xyz = grouping_operation(xyz_t, idx_knn) # (B,3,N,k)
        neigh_xyz = neigh_xyz.permute(0, 2, 3, 1)      # (B,N,k,3)

        # ä¸­å¿ƒç‚¹åæ ‡: (B,N,1,3) -> (B,N,k,3)
        center_xyz = xyz.unsqueeze(2).expand(-1, -1, self.k, -1)

        # 4) æ„é€ è¾¹ç‰¹å¾ e_ij = [h_i, h_j - h_i, x_j - x_i]
        edge_feat = torch.cat(
            [center_h, neigh_h - center_h, neigh_xyz - center_xyz],
            dim=-1
        )  # (B,N,k,2C+3)

        B_, N_, K_, D_ = edge_feat.shape
        edge_flat = edge_feat.reshape(B_ * N_ * K_, D_)  # (B*N*k, 2C+3)

        # 5) è®¡ç®—æ³¨æ„åŠ›æƒé‡ Î±_ij
        scores = self.edge_mlp_attn(edge_flat).view(B_, N_, K_)  # (B,N,k)
        alpha = F.softmax(scores, dim=-1)                        # (B,N,k)

        # 6) ç”Ÿæˆé‚»å±…æ¶ˆæ¯å¹¶èšåˆ
        msg_flat = self.edge_mlp_msg(edge_flat).view(B_, N_, K_, C)  # (B,N,k,C)
        msg = (alpha.unsqueeze(-1) * msg_flat).sum(dim=2)            # (B,N,C)

        # 7) æ®‹å·®æ›´æ–°
        h_out = h + msg
        return h_out
class TopoReasoner(nn.Module):
    """
    æ‹“æ‰‘å›¾æ¨ç†æ¨¡å—ï¼ˆTopoReasonerï¼‰

    å¯¹åº”å‰é¢è®¾è®¡æ€æƒ³ä¸­çš„å››ä¸ªéƒ¨åˆ†ï¼š
      1) å›¾æ„å»ºï¼ˆGraph Constructionï¼‰ï¼šä»¥ç‚¹åæ ‡ xyz ä¸ºèŠ‚ç‚¹ï¼ŒåŸºäº kNN å»ºå›¾
      2) ç‰¹å¾åµŒå…¥ï¼ˆFeature Embeddingï¼‰ï¼šå°† [åæ ‡, é¢å¤–ç‰¹å¾] æ˜ å°„åˆ°éšè—ç‰¹å¾ç©ºé—´
      3) æ‹“æ‰‘æ¶ˆæ¯ä¼ é€’ï¼ˆTopological Message Passingï¼‰ï¼šå¤šå±‚ GraphTopoLayer
      4) å‡ ä½•æ®‹å·®é¢„æµ‹ï¼ˆGeometric Residual Predictionï¼‰ï¼šè¾“å‡ºæ¯ä¸ªèŠ‚ç‚¹çš„ä½ç§» Î”x

    è¾“å…¥:
      xyz:        (B, N, 3)   èŠ‚ç‚¹åæ ‡ï¼ˆä¾‹å¦‚ä¸­é—´å±‚è¡¥å…¨ç‚¹ p2ï¼‰
      extra_feat: (B, N, Cg)  èŠ‚ç‚¹é™„åŠ ç‰¹å¾ï¼ˆä¾‹å¦‚å¹¿æ’­åçš„å…¨å±€ç‰¹å¾ï¼‰ï¼Œå¯ä¸º None

    è¾“å‡º:
      refined_xyz: (B, N, 3)  ä¿®æ­£åçš„ç‚¹åæ ‡
      delta_xyz:   (B, N, 3)  æ¯ä¸ªç‚¹çš„ä½ç§» Î”x
    """
    def __init__(self,
                 in_dim,          # èŠ‚ç‚¹è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆ3 + Cgï¼‰
                 hidden_dim=128,  # å›¾ä¸­éšè—ç‰¹å¾ç»´åº¦
                 k=16,            # k è¿‘é‚»é‚»å±…æ•°
                 num_layers=2):   # æ‹“æ‰‘æ¶ˆæ¯ä¼ é€’å±‚æ•°
        super().__init__()
        self.k = k
        self.num_layers = num_layers

        # 1) ç‰¹å¾åµŒå…¥ï¼šå°† [xyz, extra_feat] -> éšè—ç‰¹å¾ h
        self.node_mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True)
        )

        # 2) å¤šå±‚æ‹“æ‰‘æ¶ˆæ¯ä¼ é€’å±‚ï¼ˆæ¯å±‚éƒ½æ˜¯ä¸€ä¸ª GraphTopoLayerï¼‰
        self.layers = nn.ModuleList([
            GraphTopoLayer(hidden_dim=hidden_dim, k=k)
            for _ in range(num_layers)
        ])

        # 3) å‡ ä½•æ®‹å·®é¢„æµ‹å±‚ï¼šä»æœ€ç»ˆèŠ‚ç‚¹ç‰¹å¾ h_L ä¸­é¢„æµ‹ä½ç§» Î”x
        self.delta_mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 3)
        )

    def forward(self, xyz, extra_feat=None):
        """
        xyz:        (B, N, 3)
        extra_feat: (B, N, Cg) æˆ– None
        """
        B, N, _ = xyz.shape

        # èŠ‚ç‚¹è¾“å…¥ç‰¹å¾ï¼šå¦‚æœæœ‰é¢å¤–ç‰¹å¾ï¼Œåˆ™æ‹¼æ¥ [xyz, extra_feat]ï¼›å¦åˆ™ä»…ç”¨ xyz
        if extra_feat is not None:
            node_feat = torch.cat([xyz, extra_feat], dim=-1)  # (B,N,3+Cg)
        else:
            node_feat = xyz                                   # (B,N,3)

        # 1) ç‰¹å¾åµŒå…¥
        h = self.node_mlp(node_feat)   # (B,N,H)

        # 2) L å±‚æ‹“æ‰‘æ¶ˆæ¯ä¼ é€’
        for layer in self.layers:
            h = layer(xyz, h)          # æ¯å±‚éƒ½åœ¨å½“å‰åæ ‡ xyz ä¸Šå»ºå›¾

        # 3) æ ¹æ®æœ€ç»ˆç‰¹å¾é¢„æµ‹æ¯ä¸ªç‚¹çš„ä½ç§» Î”x
        delta_xyz = self.delta_mlp(h)  # (B,N,3)

        # 4) å åŠ ä½ç§»å¾—åˆ°ä¿®æ­£åçš„åæ ‡
        refined_xyz = xyz + delta_xyz  # (B,N,3)

        return refined_xyz, delta_xyz
class TopoCRAPCN(nn.Module):
    #æŠŠæ‹“æ‰‘å—åµŒåˆ°åˆ° CRA-PCN é‡Œï¼Œå¹¶æŠŠæ®‹å·®ä¼ åˆ° dense ç‚¹ï¼ˆp3ï¼‰ã€‚
    """
    å¸¦æ‹“æ‰‘å›¾æ¨ç†æ¨¡å—çš„ CRA-PCNï¼šTopoCRAPCN

    æ•´ä½“æµç¨‹ï¼š
      - ä»ä½¿ç”¨ CRA-PCN çš„ Encoder / SeedGenerator / Decoder ä½œä¸ºå‡ ä½•ä¸»å¹²
      - åœ¨ä¸­é—´å±‚ç‚¹äº‘ï¼ˆè¿™é‡Œé€‰ p2ï¼‰ä¸Šæ„å»ºå›¾ï¼Œå¹¶ç”¨ TopoReasoner è¿›è¡Œæ‹“æ‰‘æ¨ç†
      - å¾—åˆ° p2 ä¸Šçš„å‡ ä½•ä½ç§» Î”xï¼Œå¹¶æ›´æ–° p2 -> p2_refined
      - å†å°† p2 çš„ä½ç§»ä¼ æ’­åˆ°æœ€ç»ˆç¨ å¯†ç‚¹äº‘ p3ï¼Œå¾—åˆ° p3_refined

    è¿™æ ·å°±å¯¹åº”äº†è®¾è®¡æ€æƒ³ä¸­çš„ç¬¬äº”æ­¥ï¼š
      5) å‡ ä½•ä¿®æ­£å‘ dense ç‚¹ä¼ æ’­ï¼ˆPropagation to Dense Layerï¼‰
    """
    def __init__(self,
                 topo_hidden_dim=128,
                 topo_k=16,
                 topo_layers=2,
                 delta_scale=0.2):   # æ–°å¢ï¼šç¼©æ”¾ç³»æ•°ï¼Œæ§åˆ¶ä½ç§»å¤§å°
        super().__init__()

        self.delta_scale = delta_scale

        self.encoder = Encoder()
        self.seed_generator = SeedGenerator()
        self.decoder = Decoder()

        self.global_feat_dim = 512

        topo_in_dim = 3 + self.global_feat_dim
        self.topo_reasoner = TopoReasoner(
            in_dim=topo_in_dim,
            hidden_dim=topo_hidden_dim,
            k=topo_k,
            num_layers=topo_layers
        )
   

    def forward(self, xyz, return_all=False):
        """
        xyz: (B, N_in, 3) æ®‹ç¼ºç‚¹äº‘ï¼ˆpartialï¼‰

        è¿”å›ï¼š
          - return_all == False:
              [p_sd, p1, p2_refined, p3_refined]
          - return_all == True:
              ä¸€ä¸ªåŒ…å«ä¸­é—´å˜é‡çš„å­—å…¸ï¼Œä¾¿äºå¯è§†åŒ–å’Œè®¾è®¡æ‹“æ‰‘æŸå¤±
        """
        from models.utils import query_knn, grouping_operation  # ç”¨äºå°†ä½ç§»ä» p2 ä¼ æ’­åˆ° p3

        # -------- 1) ç¼–ç  + ç§å­ç”Ÿæˆ + è§£ç ï¼ˆä¸åŸ CRAPCN ç›¸åŒï¼‰ --------
        # Encoder æœŸæœ›è¾“å…¥å½¢çŠ¶ä¸º (B,3,N)ï¼Œå› æ­¤éœ€è¦è½¬ç½®
        pp, fp, global_f = self.encoder(xyz.permute(0, 2, 1).contiguous())
        # global_f: (B, 512, 1)  å…¨å±€è¯­ä¹‰ç‰¹å¾

        # ç§å­ç”Ÿæˆï¼šp_sd æ˜¯æœ€ç²—å±‚çš„ç‚¹äº‘ï¼Œf_sd æ˜¯å…¶ç‰¹å¾ï¼Œp0 æ˜¯ä¸­é—´ç‚¹äº‘
        p_sd, f_sd, p0 = self.seed_generator(global_f, pp, fp, xyz)

        # Decoder ç”Ÿæˆå¤šå°ºåº¦è¡¥å…¨ç»“æœï¼šall_pc = [p_sd, p1, p2, p3]
        all_pc = self.decoder(global_f, p0, p_sd, f_sd)
        # è¿™äº›ç‚¹äº‘çš„å½¢çŠ¶å‡ä¸º (B, Ni, 3)
        p_sd_xyz = all_pc[0]   # æœ€ç²—ç§å­ç‚¹
        p1_xyz   = all_pc[1]   # ç¬¬ä¸€å±‚ upsampling ç»“æœ
        p2_xyz   = all_pc[2]   # ç¬¬äºŒå±‚ï¼ˆä¸­é—´å±‚ï¼‰ç‚¹äº‘ â€”â€” æˆ‘ä»¬åœ¨è¿™é‡Œåšæ‹“æ‰‘æ¨ç†
        p3_xyz   = all_pc[3]   # æœ€ç¨ å¯†çš„è¾“å‡ºç‚¹äº‘

        B, N2, _ = p2_xyz.shape
        _, N3, _ = p3_xyz.shape

        # -------- 2) æ„é€  p2 èŠ‚ç‚¹çš„é¢å¤–ç‰¹å¾ï¼šå¹¿æ’­å…¨å±€ç‰¹å¾ --------
        # global_f: (B,512,1) -> (B,512) -> (B,1,512) -> (B,N2,512)
        global_vec = global_f.squeeze(-1)                        # (B,512)
        global_feat_expand = global_vec.unsqueeze(1).repeat(1, N2, 1)  # (B,N2,512)

        # -------- 3) åœ¨ p2 ä¸Šåšæ‹“æ‰‘å›¾æ¨ç†ï¼Œå¾—åˆ°ä¿®æ­£åçš„ p2_refined --------
        
        # åŸå§‹æ‹“æ‰‘æ¨ç†
        p2_raw_refined, delta_p2_raw = self.topo_reasoner(
            xyz=p2_xyz,
            extra_feat=global_feat_expand
        )

        # --- æ–°å¢ï¼šå»æ‰æ¯ä¸ªæ ·æœ¬çš„å…¨å±€ä½ç§»åˆ†é‡ï¼Œä½¿ Î”x æ€»å’Œä¸º 0 ---
        delta_p2_centered = delta_p2_raw - delta_p2_raw.mean(dim=1, keepdim=True)  # (B,N2,3)

        # ç„¶åå†ç¼©æ”¾ï¼Œé™åˆ¶å¹…åº¦
        delta_p2 = self.delta_scale * delta_p2_centered
        p2_refined = p2_xyz + delta_p2

      



        # -------- 4) å°† p2 ä¸Šçš„ä½ç§» Î”x ä¼ æ’­åˆ°æœ€ç»ˆå±‚ p3 --------
        # æ€è·¯ï¼š
        #   - æŠŠ p2_refined ä½œä¸ºâ€œæ”¯æŒç‚¹â€ï¼ˆsupportï¼‰ï¼Œp3 ä½œä¸ºâ€œæŸ¥è¯¢ç‚¹â€ï¼ˆqueryï¼‰
        #   - ä½¿ç”¨ query_knn åœ¨ p2_refined ä¸Šä¸ºæ¯ä¸ª p3 ç‚¹æ‰¾åˆ°æœ€è¿‘çš„ k=1 ä¸ªé‚»å±…
        #   - ç”¨ grouping_operation å–å‡ºè¿™äº›é‚»å±…çš„ä½ç§» delta_p2ï¼Œå¹¶ï¼ˆå¯é€‰ï¼‰åšå¹³å‡

        
       # -------- 4) å°† p2 ä¸Šçš„ä½ç§» Î”x ä¼ æ’­åˆ°æœ€ç»ˆå±‚ p3 --------
        # è¿™é‡Œ query_knn æœŸæœ›çš„ä¹Ÿæ˜¯ (B,N,3) å½¢å¼çš„åæ ‡
        # support = p2_refined (B,N2,3), query = p3_xyz (B,N3,3)
        idx_p3 = query_knn(1, p2_refined, p3_xyz)                 # (B,N3,1)

        # delta_p2: (B,N2,3) -> (B,3,N2) ä½œä¸ºâ€œç‰¹å¾â€ä¾› grouping_operation ä½¿ç”¨
        delta_p2_t = delta_p2.permute(0, 2, 1).contiguous()       # (B,3,N2)

        # åœ¨ p2 ä¸ŠæŒ‰ç…§ idx_p3 å–å‡ºæœ€è¿‘é‚»çš„ä½ç§»ï¼šå¾—åˆ° (B,3,N3,1)
        delta_p3_neigh = grouping_operation(delta_p2_t, idx_p3)   # (B,3,N3,1)

        # å»æ‰æœ€åä¸€ç»´ -> (B,3,N3)ï¼Œå†è½¬æˆ (B,N3,3)
        delta_p3_t = delta_p3_neigh.squeeze(-1)                   # (B,3,N3)
        delta_p3 = delta_p3_t.permute(0, 2, 1).contiguous()       # (B,N3,3)

        # æœ€ç»ˆä¿®æ­£åçš„ç¨ å¯†ç‚¹äº‘
        p3_refined = p3_xyz + delta_p3


        # -------- 5) ç»„ç»‡è¾“å‡º --------
        if not return_all:
            # è¿”å›å¤šå°ºåº¦ç‚¹äº‘ï¼Œå…¶ä¸­ p2/p3 å·²ç»åŒ…å«æ‹“æ‰‘ä¿®æ­£
            return [p_sd_xyz, p1_xyz, p2_refined, p3_refined]
        else:
            # è¿”å›å¸¦æ›´å¤šä¸­é—´é‡çš„å­—å…¸ï¼Œä¾¿äºåç»­è®¾è®¡æ‹“æ‰‘æŸå¤±æˆ–å¯è§†åŒ–
            return {
                "p_sd": p_sd_xyz,
                "p1": p1_xyz,
                "p2_raw": p2_xyz,
                "p2_refined": p2_refined,
                "p3_raw": p3_xyz,
                "p3_refined": p3_refined,
                "delta_p2": delta_p2,
                "delta_p3": delta_p3,
            }

class TopoCRAPCN_V2(nn.Module):
    """
    ä½¿ç”¨ TopoReasoningBlockV2 çš„ CRA-PCN ç‰ˆæœ¬ï¼š
      - backbone: Encoder / SeedGenerator / Decoderï¼ˆå…¨éƒ¨åŠ è½½ PCN / CRA-PCN é¢„è®­ç»ƒæƒé‡ï¼‰
      - åœ¨ä¸­é—´å±‚ç‚¹äº‘ p2 ä¸Šåšæ‹“æ‰‘æ¨ç†ï¼Œå¾—åˆ°ä½ç§» Î”x
      - å°† Î”x æ–½åŠ åˆ° p2ï¼Œå¹¶é€šè¿‡æœ€è¿‘é‚»ä¼ æ’­åˆ°æœ€ç»ˆç¨ å¯†ç‚¹ p3
      - é¢å¤–è¾“å‡º p2 çš„ backbone ç‰¹å¾ feat_p2 ä¸ topo ç‰¹å¾ topo_featï¼Œä¾¿äºåšç‰¹å¾ä¸€è‡´æ€§æŸå¤±
    """
    def __init__(self,
                 topo_hidden_dim=128,
                 topo_k=16,
                 delta_scale=0.2):
        super().__init__()

        self.delta_scale = delta_scale

        # ---- CRA-PCN ä¸»å¹² ----
        self.encoder = Encoder()
        self.seed_generator = SeedGenerator()
        self.decoder = Decoder()

        # Encoder è¾“å‡ºçš„å…¨å±€ç‰¹å¾ç»´åº¦ï¼ˆl3_pointsï¼‰
        self.global_feat_dim = 512

        # p2 ä¸Šå±€éƒ¨è¯­ä¹‰ç‰¹å¾çš„ç»´åº¦ï¼š
        # PN çš„è¾“å‡ºé€šé“å›ºå®šä¸º 128ï¼Œåœ¨ Decoder çš„ ub1 ä¸­å·²ç»å®ä¾‹åŒ–è¿‡ä¸€ä¸ª PNï¼›
        # è¿™é‡Œç›´æ¥å…±äº«è¿™ä»½æƒé‡ï¼Œä¿è¯ä½¿ç”¨çš„æ˜¯ backbone é‡Œå·²æœ‰çš„ç‰¹å¾ã€‚
        self.semantic_dim = 128
        self.pn_p2 = self.decoder.ub1.pn  # å…±äº« ub1 çš„ PN æƒé‡

        # æ‹“æ‰‘æ¨ç†æ¨¡å—ï¼šè¾“å…¥ feat_dim=128 çš„å±€éƒ¨ç‰¹å¾ + global_feat
        self.topo_reasoner = TopoReasoningBlockV2(
            feat_dim=self.semantic_dim,
            global_dim=self.global_feat_dim,
            hidden_dim=topo_hidden_dim,
            k=topo_k,
            num_layers=3,   # âœ… ä¸‰å±‚ DGCï¼Œå¤šå±‚æ¶ˆæ¯ä¼ é€’
        )

    def forward(self, xyz, return_all=False):
        """
        xyz: (B, N_in, 3) æ®‹ç¼ºç‚¹äº‘ï¼ˆpartialï¼‰

        return_all == False:
            è¿”å› [p_sd, p1, p2_refined, p3_refined]
        return_all == True:
            è¿”å›åŒ…å«ä¸­é—´ç»“æœçš„ dictï¼Œä¾›è®­ç»ƒæ—¶è®¡ç®—å„ç§æŸå¤±ï¼š
              - p2_raw / p2_refined
              - p3_raw / p3_refined
              - delta_p2 / delta_p3
              - feat_p2_backbone / topo_feat
        """
        from models.utils import query_knn, grouping_operation

        # -------- 1) CRA-PCN åŸå§‹å‰å‘ï¼ˆä¸å¸¦ topoï¼‰ --------
        # Encoder æœŸæœ›è¾“å…¥ä¸º (B,3,N)
        pp, fp, global_f = self.encoder(xyz.permute(0, 2, 1).contiguous())
        # global_f: (B,512,1)

        p_sd, f_sd, p0 = self.seed_generator(global_f, pp, fp, xyz)
        all_pc = self.decoder(global_f, p0, p_sd, f_sd)
        # all_pc: [p_sd, p1, p2, p3]ï¼Œæ¯ä¸ªéƒ½æ˜¯ (B,Ni,3)
        p_sd_xyz = all_pc[0]
        p1_xyz   = all_pc[1]
        p2_xyz   = all_pc[2]
        p3_xyz   = all_pc[3]

        B, N2, _ = p2_xyz.shape
        _, N3, _ = p3_xyz.shape

        # -------- 2) æ„é€  p2 çš„å±€éƒ¨è¯­ä¹‰ç‰¹å¾ feat_p2 --------
        # PN æœŸæœ›è¾“å…¥ xyz: (B,3,N)ï¼Œglobal_feat: (B,512,1)
        p2_xyz_t = p2_xyz.permute(0, 2, 1).contiguous()          # (B,3,N2)
        feat_p2_conv = self.pn_p2(p2_xyz_t, global_f)            # (B,128,N2)
        feat_p2 = feat_p2_conv.permute(0, 2, 1).contiguous()     # (B,N2,128)

        # åŒæ—¶å‡†å¤‡å…¨å±€å‘é‡
        global_vec = global_f.squeeze(-1)                        # (B,512)

        # -------- 3) åœ¨ p2 ä¸Šåšæ‹“æ‰‘æ¨ç†ï¼Œå¾—åˆ° Î”x å’Œ topo_feat --------
        delta_raw, score, topo_feat = self.topo_reasoner(
            xyz=p2_xyz,           # (B,N2,3)
            feat=feat_p2,         # (B,N2,128)
            global_feat=global_vec,  # (B,512)
        )                           # delta_raw: (B,N2,3), score: (B,N2,1), topo_feat: (B,N2,H)

        # å»æ‰æ ·æœ¬çº§æ•´ä½“å¹³ç§»åˆ†é‡ï¼Œä½¿æ¯ä¸ªæ ·æœ¬çš„ Î”x å’Œä¸º 0
        delta_centered = delta_raw - delta_raw.mean(dim=1, keepdim=True)  # (B,N2,3)

        # ç¼©æ”¾ + æŒ‰ç½®ä¿¡åº¦è°ƒèŠ‚
        delta_p2 = self.delta_scale * delta_centered * score              # (B,N2,3)
        p2_refined = p2_xyz + delta_p2                                    # (B,N2,3)

        # -------- 4) å°† Î”x ä» p2 ä¼ æ’­åˆ° p3 --------
        idx_p3 = query_knn(1, p2_refined, p3_xyz)                 # (B,N3,1)

        # delta_p2: (B,N2,3) -> (B,3,N2)
        delta_p2_t = delta_p2.permute(0, 2, 1).contiguous()       # (B,3,N2)

        # åœ¨ p2 ä¸ŠæŒ‰ç…§ idx_p3 å–å‡ºæœ€è¿‘é‚»çš„ä½ç§»ï¼šå¾—åˆ° (B,3,N3,1)
        delta_p3_neigh = grouping_operation(delta_p2_t, idx_p3)   # (B,3,N3,1)

        # å»æ‰æœ€åä¸€ç»´ -> (B,3,N3) -> (B,N3,3)
        delta_p3_t = delta_p3_neigh.squeeze(-1)                   # (B,3,N3)
        delta_p3 = delta_p3_t.permute(0, 2, 1).contiguous()       # (B,N3,3)

        p3_refined = p3_xyz + delta_p3                            # (B,N3,3)

        # -------- 5) ç»„ç»‡è¾“å‡º --------
        if not return_all:
            # è¿”å›å¤šå°ºåº¦ç‚¹äº‘ï¼Œå…¶ä¸­ p2/p3 å·²ç»åŒ…å«æ‹“æ‰‘ä¿®æ­£
            return [p_sd_xyz, p1_xyz, p2_refined, p3_refined]
        else:
            # è¿”å›å¸¦æ›´å¤šä¸­é—´é‡çš„å­—å…¸ï¼Œæ–¹ä¾¿è®­ç»ƒæ—¶å–ç‰¹å¾/ä½ç§»åšæŸå¤±
            return {
                "p_sd": p_sd_xyz,
                "p1": p1_xyz,
                "p2_raw": p2_xyz,
                "p2_refined": p2_refined,
                "p3_raw": p3_xyz,
                "p3_refined": p3_refined,
                "delta_p2": delta_p2,
                "delta_p3": delta_p3,
                "feat_p2_backbone": feat_p2,     # (B,N2,128)
                "topo_feat": topo_feat,          # (B,N2,H)
            }

class TopoCRAPCN_V3(nn.Module):
    """
    V3: åœ¨ V2 (TopoCRAPCN_V2) çš„åŸºç¡€ä¸Šï¼Œå¢åŠ  ESPAttention ç²¾ä¿®å±‚ã€‚

      - å…ˆç”¨ TopoCRAPCN_V2 å¾—åˆ° p3_refined å’Œ delta_p3ï¼›
      - æŠŠ [p3_refined, delta_p3] æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ï¼Œè¾“å…¥ ESPAttentionï¼›
      - åŸºäº ESP è¾“å‡ºé¢„æµ‹ç²¾ä¿®ä½ç§» delta_fineï¼Œå¾—åˆ°æœ€ç»ˆç‚¹äº‘ p3_finalã€‚
    """
    def __init__(self,
                 topo_hidden_dim=128,
                 topo_k=16,
                 topo_layers=2,      # ä¿ç•™å‚æ•°ç­¾åï¼Œæ–¹ä¾¿å…¼å®¹æ—§è„šæœ¬ï¼Œä¸å®é™…ä½¿ç”¨
                 delta_scale=0.2,
                 esp_feat_dim=64,
                 use_topo_v2=True):
        super().__init__()

        self.use_topo_v2 = use_topo_v2

        if self.use_topo_v2:
            # âœ… ä½¿ç”¨å¸¦ TopoReasoningBlockV2 çš„ç‰ˆæœ¬ä½œä¸º backbone
            self.backbone = TopoCRAPCN_V2(
                topo_hidden_dim=topo_hidden_dim,
                topo_k=topo_k,
                delta_scale=delta_scale,
            )
        else:
            # è‹¥æƒ³é€€å›æ—§çš„ GraphTopoLayer ç‰ˆæœ¬ï¼Œå¯ä»¥æŠŠ use_topo_v2=False
            self.backbone = TopoCRAPCN(
                topo_hidden_dim=topo_hidden_dim,
                topo_k=topo_k,
                topo_layers=topo_layers,
                delta_scale=delta_scale,
            )

        self.esp_feat_dim = esp_feat_dim

        # å°† [åæ ‡, coarse ä½ç§»] -> ç‰¹å¾ (B,N,6) -> (B,N,C)
        self.feat_mlp = nn.Sequential(
            nn.Linear(6, esp_feat_dim),
            nn.ReLU(inplace=True),
            nn.Linear(esp_feat_dim, esp_feat_dim),
        )

        # ESP æ³¨æ„åŠ›æ¨¡å—ï¼ˆEarth Mover / Sliced Wasserstein æ³¨æ„åŠ›ï¼‰
    

        self.esp = EspAttention(
            dim=esp_feat_dim,
            heads=4,
            dim_head=32,
            dropout=0.0,
            interp=None,
            learnable=True,
            temperature=10,
            qkv_bias=False,
            max_points=1024,  # ğŸ”¹ é™åˆ¶å‚ä¸ OT çš„ç‚¹æ•°
        )


        # ä½¿ç”¨ ESP è¾“å‡º + åæ ‡é¢„æµ‹ç²¾ä¿®ä½ç§» Î”fine
        self.delta_head = nn.Sequential(
            nn.Linear(esp_feat_dim + 3, esp_feat_dim),
            nn.ReLU(inplace=True),
            nn.Linear(esp_feat_dim, 3),
        )

    def forward(self, xyz, return_all=False):
        """
        xyz: (B, N_in, 3) æ®‹ç¼ºç‚¹äº‘
        """
        # å…ˆè·‘ V2 ä¸»å¹²ï¼Œæ‹¿åˆ° p3_refinedã€delta_p3 ç­‰ä¸­é—´ç»“æœ
        out = self.backbone(xyz, return_all=True)
        p3_refined = out["p3_refined"]                      # (B,N3,3)
        # æœ‰çš„ç‰ˆæœ¬å¯èƒ½æ²¡æœ‰ delta_p3ï¼Œè¿™é‡Œåšä¸ªå®‰å…¨å›é€€
        delta_p3 = out.get("delta_p3", torch.zeros_like(p3_refined))

        # æ„é€  ESP çš„è¾“å…¥ç‰¹å¾ï¼š[åæ ‡, coarse ä½ç§»]
        feat_in = torch.cat([p3_refined, delta_p3], dim=-1) # (B,N3,6)
        feat = self.feat_mlp(feat_in)                       # (B,N3,C)

        # é€šè¿‡ ESPAttention åšéå±€éƒ¨ç²¾ä¿®
        esp_out = self.esp(feat)                            # å¯èƒ½è¿”å› (feat,) æˆ– (feat, attn)
        if isinstance(esp_out, tuple):
            feat_esp = esp_out[0]
        else:
            feat_esp = esp_out                              # (B,N3,C)

        # å†æ‹¼å›åæ ‡ï¼Œé¢„æµ‹ç²¾ä¿®ä½ç§» Î”fine
        feat_cat = torch.cat([feat_esp, p3_refined], dim=-1)  # (B,N3,C+3)
        delta_fine = self.delta_head(feat_cat)                # (B,N3,3)
        p3_final = p3_refined + delta_fine

        if not return_all:
            # è¾“å‡ºå¤šå°ºåº¦ç‚¹äº‘åˆ—è¡¨ï¼ˆæœ€åä¸€å±‚æ¢æˆ p3_finalï¼‰
            return [out["p_sd"], out["p1"], out["p2_refined"], p3_final]
        else:
            # åœ¨ V2 çš„ä¸­é—´ç»“æœåŸºç¡€ä¸Šè¿½åŠ  V3 çš„è¾“å‡º
            out_v3 = dict(out)
            out_v3["p3_final"] = p3_final
            out_v3["delta_fine"] = delta_fine
            return out_v3
